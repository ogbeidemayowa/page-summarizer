{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pagesummarizer2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogbeidemayowa/page-summarizer/blob/master/pagesummarizer2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwnZ6Ecz_DZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4 as BeautifulSoup\n",
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGlFcroA_R1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetching the content from the URL\n",
        "fetched_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Machine_learning')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5PuQgfc_Zfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_read = fetched_data.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbLKOT5V_dDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parsing the URL content and storing in a variable\n",
        "article_parsed = BeautifulSoup.BeautifulSoup(article_read,'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hZ6ATH_iN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returning <p> tags\n",
        "paragraphs = article_parsed.find_all('p')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWCRCmGe_n_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_content = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivexmNx_u3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Looping through the paragraphs and adding them to the variable\n",
        "for p in paragraphs:  \n",
        "    article_content += p.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wb0FjAEm_0Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q45OIYhhABLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _create_dictionary_table(text_string) -> dict:\n",
        "   \n",
        "    # Removing stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    \n",
        "    words = word_tokenize(text_string)\n",
        "    \n",
        "    # Reducing words to their root form\n",
        "    stem = PorterStemmer()\n",
        "    \n",
        "    # Creating dictionary for the word frequency table\n",
        "    frequency_table = dict()\n",
        "    for wd in words:\n",
        "        wd = stem.stem(wd)\n",
        "        if wd in stop_words:\n",
        "            continue\n",
        "        if wd in frequency_table:\n",
        "            frequency_table[wd] += 1\n",
        "        else:\n",
        "            frequency_table[wd] = 1\n",
        "    return frequency_table"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhewvyOfAPvV",
        "colab_type": "code",
        "outputId": "6f4dc5cc-a4b9-4683-bac0-a8106667c2a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(article_content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Vv7L3oAV9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _calculate_sentence_scores(sentences, frequency_table) -> dict:   \n",
        "\n",
        "    # Algorithm for scoring a sentence by its words\n",
        "    sentence_weight = dict()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
        "        sentence_wordcount_without_stop_words = 0\n",
        "        for word_weight in frequency_table:\n",
        "            if word_weight in sentence.lower():\n",
        "                sentence_wordcount_without_stop_words += 1\n",
        "                if sentence[:7] in sentence_weight:\n",
        "                    sentence_weight[sentence[:7]] += frequency_table[word_weight]\n",
        "                else:\n",
        "                    sentence_weight[sentence[:7]] = frequency_table[word_weight]\n",
        "\n",
        "        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] /        sentence_wordcount_without_stop_words\n",
        "      \n",
        "    return sentence_weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8SmREnBoi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _calculate_average_score(sentence_weight) -> int:\n",
        "   \n",
        "    # Calculating the average score for the sentences\n",
        "    sum_values = 0\n",
        "    for entry in sentence_weight:\n",
        "        sum_values += sentence_weight[entry]\n",
        "\n",
        "    # Getting sentence average value from source text\n",
        "    average_score = (sum_values / len(sentence_weight))\n",
        "\n",
        "    return average_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUkdgRhbBuHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_article_summary(sentences, sentence_weight, threshold):\n",
        "    sentence_counter = 0\n",
        "    article_summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (threshold):\n",
        "            article_summary += \" \" + sentence\n",
        "            sentence_counter += 1\n",
        "\n",
        "    return article_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx9UiWTPB_KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _run_article_summary(article):\n",
        "    \n",
        "    #creating a dictionary for the word frequency table\n",
        "    frequency_table = _create_dictionary_table(article)\n",
        "\n",
        "    #tokenizing the sentences\n",
        "    sentences = sent_tokenize(article)\n",
        "\n",
        "    #algorithm for scoring a sentence by its words\n",
        "    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)\n",
        "\n",
        "    #getting the threshold\n",
        "    threshold = _calculate_average_score(sentence_scores)\n",
        "\n",
        "    #producing the summary\n",
        "    article_summary = _get_article_summary(sentences, sentence_scores, 1.5 * threshold)\n",
        "\n",
        "    return article_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufiG-A2vC9_r",
        "colab_type": "code",
        "outputId": "d042d2c4-dc89-4f54-dc81-2ff14aeb6639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    summary_results = _run_article_summary(article_content)\n",
        "    print(summary_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " In supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs. In unsupervised learning, the algorithm builds a mathematical model from a set of data which contains only inputs and no desired output labels. [8] \n",
            "As a scientific endeavor, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. [19] The data is known as training data, and consists of a set of training examples. The algorithms therefore learn from test data that has not been labeled, classified or categorized. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). In supervised feature learning, features are learned using labeled input data. In unsupervised feature learning, features are learned with unlabeled input data. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. However, over time, attention moved to performing specific tasks, leading to deviations from biology. It is one of the predictive modeling approaches used in statistics, data mining and machine learning. In machine learning, genetic algorithms were used in the 1980s and 1990s. [50]\n",
            "Usually, machine learning models require a lot of data in order for them to perform well. However, these rates are ratios that fail to reveal their numerators and denominators.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi8BO_kFKYZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}